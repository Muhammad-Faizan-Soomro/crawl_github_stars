# ğŸ•¸ï¸ GitHub Stars Crawler

A lightweight **GitHub GraphQL crawler** that collects repository metadata, stores it in **PostgreSQL**, and exports results as CSV â€” fully automated via **GitHub Actions**.

---

## âš™ï¸ Overview

This project:

* Fetches repositories (excluding forks) via **GitHub GraphQL API**
* Stores them in a **PostgreSQL** table
* Exports results to **`repositories.csv`**
* Runs automatically or manually using **GitHub Actions**

---

## ğŸ—‚ï¸ Structure

```
crawl_github_stars/
â”‚
â”œâ”€â”€ core/                     # Business logic (use cases, entities)
â”‚   â”œâ”€â”€ crawler.py            # Crawling orchestration logic
â”‚   â”œâ”€â”€ queries.py            # GraphQL queries
â”‚   â””â”€â”€ models.py             # Domain models (Repository etc.)
â”‚
â”œâ”€â”€ infrastructure/           # Low-level details (DB, API, etc.)
â”‚   â”œâ”€â”€ github_api.py         # GitHub GraphQL communication
â”‚   â”œâ”€â”€ database.py           # PostgreSQL connection + upsert
â”‚   â””â”€â”€ setup_schema.sql      # DB schema
â”‚
â”œâ”€â”€ config/                   # Configuration and environment
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ app/                      
â”‚   â””â”€â”€ main.py               # Entry point (main)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ crawl.yml         # GitHub Actions automation
```

---

## ğŸš€ Run Options

### ğŸ§‘â€ğŸ’» Locally

```bash
pip install -r requirements.txt
python app/main.py
```

### âš™ï¸ On GitHub Actions

Go to **Actions â†’ GitHub Crawler â†’ Run workflow**, wait for completion, and download `repositories.csv` from the **Artifacts** section.

---

## ğŸ“¦ Environment Variables

| Variable       | Description                  |
| -------------- | ---------------------------- |
| `DATABASE_URL` | PostgreSQL connection string |
| `GITHUB_TOKEN` | GitHub token for API access  |

---

## ğŸ§  Scaling to 500 Million Repositories

If scaling up to **500M repositories**, key changes would include:

* **Distributed Architecture:** Use multiple worker nodes (e.g., via AWS Lambda or Kubernetes) to parallelize data collection.
* **Batch Processing:** Implement sharded queries and queue-based coordination (e.g., SQS, Kafka).
* **Incremental Crawling:** Continuously update only changed repositories (using `updatedAt` field).
* **Efficient Storage:** Replace PostgreSQL with horizontally scalable DBs (e.g., BigQuery, ClickHouse, or sharded Postgres).
* **Streaming & Compression:** Stream results to cloud storage (e.g., S3) instead of holding all in memory.
* **Leverage Public Datasets:** Instead of crawling repositories directly, use publicly available GitHub datasets (e.g., on BigQuery or GH Archive) to pre-fetch `node_id`s and only query detailed metadata for relevant repositories.

---

## ğŸ§± Schema Evolution for Richer Metadata

To support **issues, PRs, commits, comments, and reviews**, evolve the schema into a **normalized, relational model**:

* **Separate tables:**

  * `repositories`, `issues`, `pull_requests`, `commits`, `comments`, `reviews`, `checks`
* **Foreign keys:** Link each child entity to its parent (`repo_id`, `pr_id`, `issue_id`, etc.)
* **Incremental updates:** Use unique IDs and timestamps (`updated_at`) to upsert only modified records â€” ensuring minimal row changes.
* **Partitioning:** Use time-based or repo-based partitions for scalable updates.
* **Indexing:** Add indexes on foreign keys and timestamps to optimize delta updates.

This structure ensures efficient tracking of evolving entities like new comments or CI results without full-table rewrites.

---

## ğŸ§¾ Summary

**GitHub Stars Crawler** automates repository data collection with a clean schema, reliable automation, and scalable design principles â€” ready to extend toward large-scale GitHub analytics.